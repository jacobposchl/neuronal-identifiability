# Deformation‑Based Neuron Identification — Scientific Synthesis

## 1. Objective
We test whether **neuron identity** (functional type) can be recovered from spiking data by linking each neuron’s activity to **geometric deformation signals** of a latent 3D dynamical system. The synthetic ground truth assigns each neuron a dominant sensitivity to one of three deformation modes:
- **Rotation** (antisymmetric component)
- **Contraction** (negative symmetric eigenvalues)
- **Expansion** (positive symmetric eigenvalues)

The central question: **Do deformation‑based features enable reliable, interpretable clustering of neuron types, and do they outperform common baselines?**

---

## 2. Synthetic Data Generation (Ground Truth)
### 2.1 Latent dynamics
A 3D latent trajectory is generated with time‑varying regimes (rotational, contraction, mixed, etc.). For each time point, the local Jacobian of the vector field is computed (finite differences), then decomposed into:
- **A (antisymmetric)** → rotation magnitude $\lVert A \rVert_F$
- **S (symmetric)** → eigenvalues; negative part defines contraction, positive part defines expansion

### 2.2 Neural population model
Each neuron has a weight vector over four drives: rotation, contraction, expansion, and a latent‑state encoding term. A “specialization” parameter controls whether neurons are strongly dominated by one drive or exhibit mixed selectivity. Spikes are generated by Poisson sampling from time‑varying firing rates with baseline, gain, noise, and adaptation.

This yields ground‑truth class labels (dominant drive) and realistic variability.

---

## 3. Feature Extraction (Method Under Test)
For each neuron, we compute **three deformation‑correlation features**:
1. Correlation with rotation magnitude
2. Correlation with contraction magnitude
3. Correlation with expansion magnitude

These three values form a low‑dimensional, interpretable feature vector per neuron. Clustering is performed using k‑means with ARI / NMI for evaluation.

**Important correction**: the deformation signals are taken **directly from the dynamics generator** (analytically correct). This avoids numerical artifacts from estimating Jacobians on noisy latents.

---

## 4. Mathematical Methodology (Formal Specification)
### 4.1 Latent dynamics and Jacobian
The latent state is a 3‑vector $z(t) \in \mathbb{R}^3$ evolving under a time‑varying vector field:
$$
\dot{z}(t) = f\big(z(t), t\big).
$$
The local linearization is given by the Jacobian:
$$
J(t) = \frac{\partial f}{\partial z}\bigg|_{z(t)}.
$$

### 4.2 Symmetric/antisymmetric decomposition
We decompose the Jacobian into symmetric and antisymmetric parts:
$$
S(t) = \tfrac{1}{2}\big(J(t) + J(t)^\top\big), \quad
A(t) = \tfrac{1}{2}\big(J(t) - J(t)^\top\big).
$$
The **rotation magnitude** is defined as the Frobenius norm:
$$
\mathrm{rot}(t) = \lVert A(t) \rVert_F.
$$
Let $\lambda_1(t) \le \lambda_2(t) \le \lambda_3(t)$ be eigenvalues of $S(t)$. We define
$$
\mathrm{con}(t) = -\sum_{\lambda_i(t) < 0} \lambda_i(t), \quad
\mathrm{exp}(t) = \sum_{\lambda_i(t) > 0} \lambda_i(t).
$$
These yield non‑negative contraction and expansion magnitudes.

### 4.3 Neural population model
Each neuron $i$ has weights $w_i = (w_{i,\mathrm{rot}}, w_{i,\mathrm{con}}, w_{i,\mathrm{exp}}, w_{i,\mathrm{state}})$.
The instantaneous drive is
$$
d_i(t) = \alpha(t)\Big[w_{i,\mathrm{rot}}\,\mathrm{rot}(t) + w_{i,\mathrm{con}}\,\mathrm{con}(t) + w_{i,\mathrm{exp}}\,\mathrm{exp}(t)\Big] + w_{i,\mathrm{state}}\,\langle u_i, z(t) \rangle,
$$
where $u_i \in \mathbb{R}^3$ encodes the neuron’s state preference and $\alpha(t)$ is a slow adaptation term. The deformation term is normalized by
$$
\alpha(t) \propto \frac{1}{1 + |\mathrm{rot}(t)| + |\mathrm{con}(t)| + |\mathrm{exp}(t)|}
$$
to prevent domination by any single deformation mode. Firing rates are
$$
r_i(t) = b_i + g_i\,d_i(t), \quad r_i(t) \ge 0,
$$
with spikes drawn from an inhomogeneous Poisson process with rate $r_i(t)$.

### 4.4 Feature construction
For each neuron, we compute three correlation features over time:
$$
\phi_i = \big[\rho(r_i, \mathrm{rot}),\ \rho(r_i, \mathrm{con}),\ \rho(r_i, \mathrm{exp})\big],
$$
where $\rho(\cdot,\cdot)$ denotes Pearson correlation. These features define the embedding for clustering.

### 4.5 Clustering and evaluation
We apply k‑means to the feature matrix and quantify recovery of ground‑truth labels using **ARI** and **NMI**, with paired t‑tests across trials for statistical comparison.

---

## 5. Evaluation Metrics
- **NMI (Normalized Mutual Information)** for global information content
- **ARI (Adjusted Rand Index)** for clustering fidelity against ground truth
- **Paired t‑tests** for statistical significance across trials

---

## 6. Test Suite Summary (11 Tests)
### Test 1 — Information Content (NMI)
**Goal:** Compare deformation features vs PCA, cross‑correlation, and dimensionality features.
**Result:** Deformation features significantly outperform all baselines (p < 1e‑4). Effect size is large and consistent.

### Test 2 — Noise Robustness (Deformation Measurement Noise)
**Goal:** Add substantial noise to deformation signals and test performance.
**Result:** ARI remains essentially stable even at 100% noise amplitude. Deformation correlations are robust to additive noise due to normalization in correlation coefficients.

### Test 3 — Clustering Stability
**Goal:** Check sensitivity to k‑means initialization.
**Result:** High mean ARI with narrow confidence interval across seeds; clustering is stable.

### Test 4 — Sufficient Statistics / Redundancy
**Goal:** Compare each deformation feature alone vs all three together, and examine inter‑feature correlations.
**Result:** 3D feature set yields a strong boost over any 1D feature (≈25% improvement). Features are correlated but not redundant; combined signal is clearly superior.

### Test 5 — Independence from Assumptions
**Goal:** Ensure the method does not produce structure on random labels.
**Result:** True labels ≫ random labels (p < 1e‑4). Low false discovery of structure.

### Test 6 — Generalization Across Population Configurations
**Goal:** Vary population size and specialization.
**Result:** High/medium specialization performs well; low specialization yields large performance drop (method struggles with weakly separable classes).

### Test 7 — Different Noise Types in Firing Rates
**Goal:** Add Gaussian, Poisson, and outlier noise to firing rates.
**Result:** Performance is robust to Gaussian/outlier noise; Poisson noise degrades more strongly but remains above chance.

### Test 8 — False Positives (Random Data)
**Goal:** Test on purely random spike trains.
**Result:** ARI near zero; low false positive rate.

### Test 9 — Alternative Feature Variants
**Goal:** Compare deformation correlations to trace‑based and norm‑based variants.
**Result:** Deformation‑correlation features are best in this synthetic setup.

### Test 10 — Latent Estimation Noise (PCA Latents)
**Goal:** Replace true latents with PCA‑estimated latents, then re‑estimate deformation.
**Result:** ~46% degradation in ARI (p ≈ 0.003). This is the **largest limitation** for real‑world applicability.

### Test 11 — Deformation Ablation
**Goal:** Zero out each deformation signal (rotation/contraction/expansion) and measure impact.
**Result:** The corresponding feature collapses (≈100% drop), and ARI decreases by ~20–30%. This validates interpretability and causal relevance of each feature.

---

## 7. Interpretation and Significance
### Strengths
- Strong statistical advantage over common baselines (Test 1).
- Highly robust to measurement noise in deformation signals (Test 2).
- Interpretable features with causal ablation support (Test 11).
- Stable clustering and low false positives (Tests 3 and 8).

### Limitations
- **Latent estimation is a bottleneck.** PCA‑based latent inference severely degrades performance (Test 10).
- Performance collapses when neuron types are weakly separated (Test 6, low specialization).

### What this means scientifically
The deformation‑based representation is **valid and powerful** in a controlled synthetic regime with known dynamics. It is a strong conceptual framework for mapping neuron identity to dynamical geometry. However, **real‑world utility will depend on reliable latent inference** (e.g., GPFA, LFADS, or dynamical systems models) and robustness to weak specialization.

---

